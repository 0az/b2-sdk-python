#!/usr/bin/env python
######################################################################
#
# File: b2
#
# Copyright 2015 Backblaze Inc. All Rights Reserved.
#
# License https://www.backblaze.com/using_b2_code.html
#
######################################################################

# Note on #! line: There doesn't seem to be one that works for
# everybody.  Most users of this program are Mac users who use the
# default Python installed in OSX, which is called "python" or
# "python2.7", but not "python2".  So we don't use "python2".

"""
This is a B2 command-line tool.  See the USAGE message for details.
"""

from abc import ABCMeta, abstractmethod
import base64
import datetime
import getpass
import hashlib
import json
import os.path
import sys
import urllib
import urllib2


try:
    from tqdm import tqdm  # displays a nice progress bar
    PROGRESS_BAR_SUPPORT = True
except ImportError:
    PROGRESS_BAR_SUPPORT = False


# To avoid confusion between official Backblaze releases of this tool and
# the versions on Github, we use the convention that the third number is
# odd for Github, and even for Backblaze releases.
VERSION = '0.3.9'

USAGE = """This program provides command-line access to the B2 service.

Usages:

    b2 authorize_account [--dev | --staging | --production] [accountId] [applicationKey]

        Prompts for Backblaze accountID and applicationKey (unless they are given
        on the command line).

        The account ID is a 12-digit hex number that you can get from
        your account page on backblaze.com.

        The application key is a 40-digit hex number that you can get from
        your account page on backblaze.com.

        Stores an account auth token in ~/.b2_account_info

    b2 clear_account

        Erases everything in ~/.b2_account_info

    b2 create_bucket <bucketName> <bucketType>

        Creates a new bucket.  Prints the ID of the bucket created.

    b2 delete_bucket <bucketName>

        Deletes the bucket with the given name.

    b2 delete_file_version <fileName> <fileId>

        Permanently and irrevocably deletes one version of a file.

    b2 download_file_by_id <fileId> <localFileName>

        Downloads the given file, and stores it in the given local file.

    b2 download_file_by_name <bucketName> <fileName> <localFileName>

        Downloads the given file, and stores it in the given local file.

    b2 get_file_info <fileId>

        Prints all of the information about the file, but not its contents.

    b2 hide_file <bucketName> <fileName>

        Uploads a new, hidden, version of the given file.

    b2 list_buckets

        Lists all of the buckets in the current account.

    b2 list_file_names <bucketName> [<startingName>] [<numberToShow>]

        Lists the names of the files in a bucket, starting at the
        given point.

    b2 list_file_versions <bucketName> [<startingName>] [<startingFileId>] [<numberToShow>]

        Lists the names of the files in a bucket, starting at the
        given point.

    b2 ls [--long] [--versions] <bucketName> [<folderName>]

        Using the file naming convention that "/" separates folder
        names from their contents, returns a list of the files
        and folders in a given folder.  If no folder name is given,
        lists all files at the top level.

        The --long option produces very wide multi-column output
        showing the upload date/time, file size, file id, whether it
        is an uploaded file or the hiding of a file, and the file
        name.  Folders don't really exist in B2, so folders are
        shown with "-" in each of the fields other than the name.

        The --version option shows all of versions of each file, not
        just the most recent.

    b2 make_url <fileId>

        Prints an URL that can be used to download the given file, if
        it is public.

    b2 update_bucket <bucketName> <bucketType>

        Updates the bucketType of an existing bucket.  Prints the ID
        of the bucket updated.

    b2 upload_file [--sha1 <sha1sum>] [--contentType <contentType>] [--info <key>=<value>]* <bucketName> <localFilePath> <b2FileName>

        Uploads one file to the given bucket.  Uploads the contents
        of the local file, and assigns the given name to the B2 file.

        By default, upload_file will compute the sha1 checksum of the file
        to be uploaded.  But, you you already have it, you can provide it
        on the command line to save a little time.

        Content type is optional.  If not set, it will be set based on the
        file extension.

        If `tqdm` library is installed, progress bar is displayed on stderr.
        (use pip install tqdm to install it)

        Each fileInfo is of the form "a=b".

    b2 version

        Echos the version number of this program.
"""


## Exceptions

class B2Error(Exception):
    pass


class DuplicateBucketName(B2Error):
    def __init__(self, bucket_name):
        self.bucket_name = bucket_name
    def __str__(self):
        return 'Bucket name is already in use: %s' % (self.bucket_name,)


class FileAlreadyHidden(B2Error):
    def __init__(self, file_name):
        self.file_name = file_name
    def __str__(self):
        return 'File already hidden: %s' % (self.file_name,)


class NonExistentBucket(B2Error):
    def __init__(self, bucket_name):
        self.bucket_name = bucket_name
    def __str__(self):
        return 'No such bucket: %s' % (self.bucket_name,)


class FileNotPresent(B2Error):
    def __init__(self, file_name):
        self.file_name = file_name
    def __str__(self):
        return 'File not present: %s' % (self.file_name,)


class UnrecognizedBucketType(B2Error):
    def __init__(self, type_):
        self.type_ = type_
    def __str__(self):
        return 'Unrecognized bucket type: %s' % (self.type_,)


## Bucket

class Bucket(object):
    __metaclass__ = ABCMeta
    def __init__(self, api, id_, name=None, type_=None):
        self.api = api
        self.id_ = id_
        self.name = name
        self.type_ = type_
    def set_type(self, type_):
        account_info = self.api.account_info
        auth_token = account_info.get_account_auth_token()
        account_id = account_info.get_account_id()

        url = url_for_api(account_info, 'b2_update_bucket')
        params = {
            'accountId' : account_id,
            'bucketId' : self.id_,
            'bucketType' : type_,
        }
        response = post_json(url, params, auth_token)
        return response
    def list_file_names(self, start_filename=None, max_entries=None):  # TODO
        return ["foo"]
    def list_file_versions(self, start_filename=None, max_entries=None):  # TODO
        return [FileVersionInfo()]
    def upload_file(self, input_stream, remote_filename, mimeTypeOrNull=None, extra_headers=None):  # TODO
        raise B2Error()
        return FileVersionInfo()
    def hide_file(self, file_name):
        account_info = self.api.account_info
        auth_token = account_info.get_account_auth_token()

        url = url_for_api(account_info, 'b2_hide_file')
        params = {
            'bucketId' : self.id_,
            'fileName' : file_name,
        }
        try:  # TODO: refactor the cloned exception handling
            response = post_json(url, params, auth_token, exit_on_error=False)
        except urllib2.HTTPError as e:
            error_content = e.read()
            try:
                error_dict = json.loads(error_content)
            except ValueError:
                self._display_error(error_content)
            status = error_dict.get('status')
            code = error_dict.get('code')
            if status == 400 and code == "already_hidden":
                raise FileAlreadyHidden(file_name)
            elif status == 400 and code in ("no_such_file", "file_not_present"):
                # hide_file returns "no_such_file"
                # delete_file_version returns "file_not_present"
                raise FileNotPresent(file_name)
            raise
        return FileVersionInfoFactory.from_api_response(response)
    def as_dict(self):  # TODO: refactor with other as_dict()
        result = {
            'accountId': self.api.account_info.get_account_id(),
            'bucketId': self.id_,
        }
        if self.name is not None:
            result['bucketName'] = self.name
        if self.type_ is not None:
            result['bucketType'] = self.type_
        return result
    def __repr__(self):
        return 'Bucket<%s,%s,%s>' % (self.id_, self.name, self.type_)


class BucketFactory(object):
    @classmethod
    def from_api_response(cls, api, response):
        return [
            cls.from_api_bucket_dict(api, bucket_dict)
            for bucket_dict in response['buckets']
        ]
    @classmethod
    def from_api_bucket_dict(cls, api, bucket_dict):
        """
            turns this:
            {
                "bucketType": "allPrivate",
                "bucketId": "a4ba6a39d8b6b5fd561f0010",
                "bucketName": "zsdfrtsazsdfafr",
                "accountId": "4aa9865d6f00"
            }
            into a Bucket object
        """
        bucket_name = bucket_dict['bucketName']
        bucket_id = bucket_dict['bucketId']
        type_ = bucket_dict['bucketType']
        if type_ is None:
            raise UnrecognizedBucketType(bucket_dict['bucketType'])
        return Bucket(api, bucket_id, bucket_name, type_)


## DAO

class FileVersionInfo(object):
    def __init__(self, id_, file_name, size, upload_timestamp, action):
        self.id_ = id_
        self.file_name = file_name
        self.size = size  # can be None (unknown)
        self.upload_timestamp = upload_timestamp  # can be None (unknown)
        self.action = action  # "upload" or "hide" or "delete"
    def as_dict(self):
        result = {
            'fileId': self.id_,
            'fileName': self.file_name,
        }

        if self.size is not None:
            result['size'] = self.size
        if self.upload_timestamp is not None:
            result['uploadTimestamp'] = self.upload_timestamp
        if self.action is not None:
            result['action'] = self.action
        return result


class FileVersionInfoFactory(object):
    @classmethod
    def from_api_response(cls, file_info_dict, force_action=None):
        """
            turns this:
            {
              "action": "hide",
              "fileId": "4_zBucketName_f103b7ca31313c69c_d20151230_m030117_c001_v0001015_t0000",
              "fileName": "randomdata",
              "size": 0,
              "uploadTimestamp": 1451444477000
            }
            into a FileVersionInfo object
        """
        assert file_info_dict.get('action') is None or force_action is None, \
            'action was provided by both info_dict and function argument'
        action = file_info_dict.get('action') or force_action
        file_name = file_info_dict['fileName']
        id_ = file_info_dict['fileId']
        size = file_info_dict.get('size')
        upload_timestamp = file_info_dict.get('uploadTimestamp')

        return FileVersionInfo(id_, file_name, size, upload_timestamp, action)


## Cache

class AbstractCache(object):
    __metaclass__ = ABCMeta
    @abstractmethod
    def get_bucket_id_or_none_from_bucket_name(self, name):
        pass
    @abstractmethod
    def save_bucket(self, bucket):
        pass
    @abstractmethod
    def set_bucket_name_cache(self, buckets):
        pass
    def _name_id_iterator(self, buckets):
        return ((bucket.name, bucket.id_) for bucket in buckets)


class DummyCache(AbstractCache):
    """ Cache that does nothing """
    def get_bucket_id_or_none_from_bucket_name(self, name):
        return None
    def save_bucket(self, bucket):
        pass
    def set_bucket_name_cache(self, buckets):
        pass


class InMemoryCache(AbstractCache):
    """ Cache that stores the information in memory """
    def __init__(self):
        self.name_id_map = {}
    def get_bucket_id_or_none_from_bucket_name(self, name):
        return self.name_id_map.get(name)
    def save_bucket(self, bucket):
        pass
    def set_bucket_name_cache(self, buckets):
        self.name_id_map = dict(self._name_id_iterator(buckets))


class AuthInfoCache(AbstractCache):
    """ Cache that stores data persistently in StoredAccountInfo """
    def __init__(self, info):
        self.info = info
    def get_bucket_id_or_none_from_bucket_name(self, name):
        return self.info.get_bucket_id_or_none_from_bucket_name(name)
    def save_bucket(self, bucket):
        self.info.save_bucket(bucket)
    def set_bucket_name_cache(self, buckets):
        self.info.refresh_entire_bucket_name_cache(self._name_id_iterator(buckets))


## B2Api

class B2Api(object):
    def __init__(self, account_info=None, cache=None):
        if account_info is None:
            account_info = StoredAccountInfo()
            if cache is None:
                cache = AuthInfoCache(account_info)
        self.account_info = account_info
        if cache is None:
            cache = DummyCache()
        self.cache = cache

    # buckets
    def create_bucket(self, name, type_):
        account_id = self.account_info.get_account_id()
        auth_token = self.account_info.get_account_auth_token()

        url = url_for_api(self.account_info, 'b2_create_bucket')
        params = {
            'accountId' : account_id,
            'bucketName' : name,
            'bucketType' : type_,
            }
        try:
            response = post_json(url, params, auth_token, exit_on_error=False)
        except urllib2.HTTPError as e:
            error_content = e.read()
            try:
                error_dict = json.loads(error_content)
            except ValueError:
                self._display_error(error_content)
            status = error_dict.get('status')
            code = error_dict.get('code')
            if status == 400 and code == "duplicate_bucket_name":
                raise DuplicateBucketName(name)
            raise
        bucket = BucketFactory.from_api_bucket_dict(self, response)
        assert name == bucket.name, 'API created a bucket with different name\
                                     than requested: %s != %s' % (name, bucket.name)
        assert type_ == bucket.type_, 'API created a bucket with different type\
                                     than requested: %s != %s' % (type_, bucket.type_)
        self.cache.save_bucket(bucket)
        return bucket
    def get_bucket_by_id(self, bucket_id):
        return Bucket(self, bucket_id)
    def get_bucket_by_name(self, bucket_name):
        """
        Returns the bucket_id for the given bucket_name.

        If we don't already know it from the cache, try fetching it from
        the B2 service.
        """
        # If we can get it from the stored info, do that.
        id_ = self.cache.get_bucket_id_or_none_from_bucket_name(bucket_name)
        if id_ is not None:
            return Bucket(self, id_, name=bucket_name)

        for bucket in self.list_buckets():
            if bucket.name == bucket_name:
                return bucket
        raise NonExistentBucket(bucket_name)

    def delete_bucket(self, bucket):
        """
        Deletes the bucket remotely.
        For legacy reasons it returns whatever server sends in response,
        but API user should not rely on the response: if it doesn't raise
        an exception, it means that the operation was a success
        """
        account_id = self.account_info.get_account_id()
        auth_token = self.account_info.get_account_auth_token()
        url = url_for_api(self.account_info, 'b2_delete_bucket')
        params = {
            'accountId' : account_id,
            'bucketId' : bucket.id_,
            }
        return post_json(url, params, auth_token)

    def list_buckets(self):
        """
        Calls b2_list_buckets and returns the JSON for *all* buckets.
        """
        account_id = self.account_info.get_account_id()
        auth_token = self.account_info.get_account_auth_token()

        url = url_for_api(self.account_info, 'b2_list_buckets')
        params = {'accountId': account_id}
        response = post_json(url, params, auth_token)

        buckets = BucketFactory.from_api_response(self, response)

        self.cache.set_bucket_name_cache(buckets)
        return buckets

    # delete
    def delete_file_version(self, file_id, file_name):  # filename argument is not first,
                                                        # because one day it may become
                                                        # optional
        auth_token = self.account_info.get_account_auth_token()

        url = url_for_api(self.account_info, 'b2_delete_file_version')

        params = {
            'fileName': file_name,
            'fileId': file_id,
        }
        response = post_json(url, params, auth_token)
        file_info = FileVersionInfoFactory.from_api_response(
            response,
            force_action='delete',
        )
        assert file_info.id_ == file_id
        assert file_info.file_name == file_name
        assert file_info.action == 'delete'
        return file_info

    # download
    def download_file_by_id(self, file_id, content_handler):
        pass
    def download_file_by_name(self, bucket_name, filename, content_handler):
        pass

    # other
    def make_url(self, file_id):
        """
        returns a download url for given file_id
        """
        #bucket_id = file_id[3:27]
        url = url_for_api(self.account_info, 'b2_download_file_by_id')
        return '%s?fileId=%s' % (url, file_id)
    def get_file_info(self, file_id):
        return FileVersionInfo()
        raise B2Error()


## v0.3.x functions

def message_and_exit(message):
    """Prints a message, and exits with error status.
    """
    print >>sys.stderr, message
    sys.exit(1)


def usage_and_exit():
    """Prints a usage message, and exits with an error status.
    """
    message_and_exit(USAGE)


def decode_sys_argv():
    """
    Returns the command-line arguments as unicode strings, decoding
    whatever format they are in.

    https://stackoverflow.com/questions/846850/read-unicode-characters-from-command-line-arguments-in-python-2-x-on-windows
    """
    encoding = sys.getfilesystemencoding()
    return [arg.decode(encoding) for arg in sys.argv]


class AbstractAccountInfo(object):
    __metaclass__ = ABCMeta
    @abstractmethod
    def get_api_url(self):
        pass
    @abstractmethod
    def get_download_url(self):
        pass


class StoredAccountInfo(AbstractAccountInfo):

    """Manages the file that holds the account ID and stored auth tokens.

    When an instance of this class is created, it reads the account
    info file in the home directory of the user, and remembers the info.

    When any changes are made, they are written out to the file.
    """

    ACCOUNT_AUTH_TOKEN = 'account_auth_token'
    ACCOUNT_ID = 'account_id'
    API_URL = 'api_url'
    BUCKET_NAMES_TO_IDS = 'bucket_names_to_ids'
    BUCKET_UPLOAD_DATA = 'bucket_upload_data'
    BUCKET_UPLOAD_URL = 'bucket_upload_url'
    BUCKET_UPLOAD_AUTH_TOKEN = 'bucket_upload_auth_token'
    DOWNLOAD_URL = 'download_url'

    def __init__(self):
        self.filename = os.path.expanduser('~/.b2_account_info')
        self.data = self._try_to_read_file()
        if self.BUCKET_UPLOAD_DATA not in self.data:
            self.data[self.BUCKET_UPLOAD_DATA] = {}
        if self.BUCKET_NAMES_TO_IDS not in self.data:
            self.data[self.BUCKET_NAMES_TO_IDS] = {}

    def clear(self):
        """Removes all stored information.
        """
        self.data = {}
        self._write_file()


    def _try_to_read_file(self):
        try:
            with open(self.filename, 'rb') as f:
                return json.loads(f.read())
        except Exception as e:
            return {}

    def get_account_id(self):
        return self._get_account_info_or_exit(self.ACCOUNT_ID)

    def get_account_auth_token(self):
        return self._get_account_info_or_exit(self.ACCOUNT_AUTH_TOKEN)

    def get_api_url(self):
        return self._get_account_info_or_exit(self.API_URL)

    def get_download_url(self):
        return self._get_account_info_or_exit(self.DOWNLOAD_URL)

    def _get_account_info_or_exit(self, key):
        """Returns the named field from the account data, or errors and exits.
        """
        result = self.data.get(key)
        if result is None:
            message_and_exit('ERROR: No account.  Use: b2 authorize_account')
        return result

    def set_account_id_and_auth_token(self, account_id, auth_token, api_url, download_url):
        self.data[self.ACCOUNT_ID] = account_id
        self.data[self.ACCOUNT_AUTH_TOKEN] = auth_token
        self.data[self.API_URL] = api_url
        self.data[self.DOWNLOAD_URL] = download_url
        self._write_file()

    def set_bucket_upload_data(self, bucket_id, upload_url, upload_auth_token):
        self.data[self.BUCKET_UPLOAD_DATA][bucket_id] = {self.BUCKET_UPLOAD_URL : upload_url, self.BUCKET_UPLOAD_AUTH_TOKEN : upload_auth_token}
        self._write_file()

    def get_bucket_upload_data(self, bucket_id):
        return self.data[self.BUCKET_UPLOAD_DATA].get(bucket_id)

    def clear_bucket_upload_data(self, bucket_id):
        upload_data = self.data[self.BUCKET_UPLOAD_DATA]
        if bucket_id in upload_data:
            del upload_data[bucket_id]

    def save_bucket(self, bucket):
        names_to_ids = self.data[self.BUCKET_NAMES_TO_IDS]
        if names_to_ids.get(bucket.name) != bucket.id_:
            names_to_ids[bucket.name] = bucket.id_
            self._write_file()

    def refresh_entire_bucket_name_cache(self, name_id_iterable):
        names_to_ids = self.data[self.BUCKET_NAMES_TO_IDS]
        new_cache = dict(name_id_iterable)
        if names_to_ids != new_cache:
            self.data[self.BUCKET_NAMES_TO_IDS] = new_cache
            self._write_file()

    def remove_bucket_name(self, bucket_name):
        names_to_ids = self.data[self.BUCKET_NAMES_TO_IDS]
        if bucket_name in names_to_ids:
            del names_to_ids[bucket_name]
        self._write_file()

    def get_bucket_id_or_none_from_bucket_name(self, bucket_name):
        names_to_ids = self.data[self.BUCKET_NAMES_TO_IDS]
        return names_to_ids.get(bucket_name)

    def _write_file(self):
        flags = os.O_WRONLY | os.O_CREAT | os.O_TRUNC
        if os.name == 'nt':
            flags |= os.O_BINARY
        with os.fdopen(os.open(self.filename, flags, 0600), 'wb') as f:
            json.dump(self.data, f, indent=4, sort_keys=True)


def report_http_error_and_exit(e, url, data, headers):
    print 'Error returned from server:'
    print
    print 'URL:', url
    print 'Params:', data
    print 'Headers:', headers
    print
    print e.read()
    sys.exit(1)


class OpenUrl(object):
    """
    Context manager that handles an open urllib2.Request, and provides
    the file-like object that is the response.
    """

    def __init__(self, url, data, headers, exit_on_error=True):
        self.url = url
        self.data = data
        self.headers = headers
        self.file = None
        self.exit_on_error = exit_on_error

    def __enter__(self):
        try:
            request = urllib2.Request(self.url, self.data, self.headers)
            self.file = urllib2.urlopen(request)
            return self.file
        except urllib2.HTTPError as e:
            if self.exit_on_error:
                report_http_error_and_exit(e, self.url, self.data, self.headers)
            else:
                raise e

    def __exit__(self, exception_type, exception, traceback):
        if self.file is not None:
            self.file.close()


def post_json(url, params, auth_token=None, exit_on_error=True):
    """Coverts params to JSON and posts them to the given URL.

    Returns the resulting JSON, decoded into a dict.
    """
    data = json.dumps(params)
    headers = {}
    if auth_token is not None:
        headers['Authorization'] = auth_token
    with OpenUrl(url, data, headers, exit_on_error) as f:
        json_text = f.read()
        return json.loads(json_text)


class StreamWithProgress(PROGRESS_BAR_SUPPORT and tqdm or object):  # this keeps pyflakes calm, unlike tqdm=object
    def __init__(self, stream, *args, **kwargs):
        self.stream = stream
        kwargs.update(
            {
                'unit': 'B',
                'unit_scale': True,
                'leave': True,
                'miniters': 1,
            }
        )
        super(StreamWithProgress, self).__init__(*args, **kwargs)

    def __enter__(self):
        super(StreamWithProgress, self).__enter__()
        self.stream.__enter__()
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        return any(
            (
                super(StreamWithProgress, self).__exit__(exc_type, exc_value, traceback),
                self.stream.__exit__(exc_type, exc_value, traceback),
            )
        )

    def read(self, size):
        data = self.stream.read(size)
        self.update(len(data))
        return data

    def write(self, data):
        self.stream.write(data)
        self.update(len(data))


def post_file(url, headers, file_path, exit_on_error=True, progress_bar=False):
    """
    Posts the contents of the local file to the given URL.
    """
    if 'Content-Length' not in headers:
        headers['Content-Length'] = str(os.path.getsize(file_path))
    stream = open(file_path, 'rb')
    if PROGRESS_BAR_SUPPORT and progress_bar:
        stream = StreamWithProgress(stream, desc=file_path, total=int(headers['Content-Length']))
    with stream as data_file:
        with OpenUrl(url, data_file, headers, exit_on_error) as response_file:
            json_text = response_file.read()
            return json.loads(json_text)


def clear_account(args):
    if len(args) != 0:
        usage_and_exit()
    info = StoredAccountInfo()
    info.clear()


def url_for_api(info, api_name):
    if api_name in ['b2_download_file_by_id']:
        base = info.get_download_url()
    else:
        base = info.get_api_url()
    return base + '/b2api/v1/' + api_name


def b2_url_encode(s):
    """URL-encodes a unicode string to be sent to B2 in an HTTP header.
    """
    return urllib.quote(s.encode('utf-8'))


def b2_url_decode(s):
    """Decodes a Unicode string returned from B2 in an HTTP header.

    Returns a Python unicode string.
    """
    # Use str() to make sure that the input to unquote is a str, not
    # unicode, which ensures that the result is a str, which allows
    # the decoding to work properly.
    return urllib.unquote_plus(str(s)).decode('utf-8')


def authorize_account(args):

    auth_urls = {'--production':'https://api.backblaze.com','--dev':'http://api.test.blaze:8180','--staging':'https://api.backblaze.net'}

    option = '--production'
    url = auth_urls[option]
    while 0 < len(args) and args[0][0] == '-':
        option = args[0]
        args = args[1:]
        if option in auth_urls:
            url = auth_urls[option]
            break
        else:
            print 'ERROR: unknown option', option
            usage_and_exit()

    print 'Using %s' % url

    if 2 < len(args):
        usage_and_exit()
    if 0 < len(args):
        accountId = args[0]
    else:
        accountId = raw_input('Backblaze account ID: ')

    if 1 < len(args):
        applicationKey = args[1]
    else:
        applicationKey = getpass.getpass('Backblaze application key: ')

    url += '/b2api/v1/b2_authorize_account'

    auth = 'Basic '+ base64.b64encode('%s:%s' % (accountId, applicationKey))
    response = post_json(url, {}, auth)

    info = StoredAccountInfo()
    info.clear()
    info.set_account_id_and_auth_token(
        response['accountId'],
        response['authorizationToken'],
        response['apiUrl'],
        response['downloadUrl']
        )


def list_file_names(args):

    if len(args) < 1 or 3 < len(args):
        usage_and_exit()

    bucket_name = args[0]
    if 2 <= len(args):
        firstFileName = args[1]
    else:
        firstFileName = None
    if 3 <= len(args):
        count = int(args[2])
    else:
        count = 100

    console_tool = ConsoleTool()
    info = console_tool.info
    api = console_tool.api
    bucket = api.get_bucket_by_name(bucket_name)
    auth_token = info.get_account_auth_token()

    url = url_for_api(info, 'b2_list_file_names')
    params = {
        'bucketId' : bucket.id_,
        'startFileName' : firstFileName,
        'maxFileCount' : count
    }
    response = post_json(url, params, auth_token)

    print json.dumps(response, indent=2, sort_keys=True)

def list_file_versions(args):

    if len(args) < 1 or 4 < len(args):
        usage_and_exit()

    bucket_name = args[0]
    if 2 <= len(args):
        firstFileName = args[1]
    else:
        firstFileName = None
    if 3 <= len(args):
        firstFileId = args[2]
    else:
        firstFileId = None
    if 4 <= len(args):
        count = int(args[3])
    else:
        count = 100

    console_tool = ConsoleTool()
    info = console_tool.info
    api = console_tool.api
    bucket = api.get_bucket_by_name(bucket_name)
    auth_token = info.get_account_auth_token()

    url = url_for_api(info, 'b2_list_file_versions')
    params = {
        'bucketId' : bucket.id_,
        'startFileName' : firstFileName,
        'startFileId' : firstFileId,
        'maxFileCount' : count
    }
    response = post_json(url, params, auth_token)

    print json.dumps(response, indent=2, sort_keys=True)


def ensure_upload_data(bucket_id, info, quiet):
    """
    Makes sure that we have an upload URL and auth token for the given bucket and
    returns it.
    """
    upload_data = info.get_bucket_upload_data(bucket_id)
    if upload_data is None:
        if not quiet:
            print 'Getting upload URL...'
        auth_token = info.get_account_auth_token()
        url = url_for_api(info, 'b2_get_upload_url')
        params = { 'bucketId' : bucket_id }
        response = post_json(url, params, auth_token)
        upload_url = response['uploadUrl']
        upload_auth_token = response['authorizationToken']
        info.set_bucket_upload_data(bucket_id, upload_url, upload_auth_token)
        upload_data = info.get_bucket_upload_data(bucket_id)
    return upload_data


def get_file_info(args):
    if len(args) != 1:
        usage_and_exit()
    file_id = args[0]
    bucket_id = file_id[3:27]

    info = StoredAccountInfo()
    auth_token = info.get_account_auth_token()

    url = url_for_api(info, 'b2_get_file_info')
    params = { 'fileId' : file_id }
    response = post_json(url, params, auth_token)

    print json.dumps(response, indent=2, sort_keys=True)

def hex_sha1_of_file(path):
    with open(path, 'rb') as f:
        block_size = 1024 * 1024
        digest = hashlib.sha1()
        while True:
            data = f.read(block_size)
            if len(data) == 0:
                break
            digest.update(data)
        return digest.hexdigest()


def parse_file_info(item, file_infos):
    parts = item.split('=')
    if len(parts) != 2:
        print >>sys.stdout, 'ERROR: bad file info:', item
        sys.exit(1)
    file_infos[parts[0]] = parts[1]


def upload_file(args):

    content_type = 'b2/x-auto'
    file_infos = {}
    sha1_sum = None
    quiet = False

    while 0 < len(args) and args[0][0] == '-':
        option = args[0]
        if option == '--sha1':
            if len(args) < 2:
                usage_and_exit()
            sha1_sum = args[1]
            args = args[2:]
        elif option == '--contentType':
            if len(args) < 2:
                usage_and_exit()
            content_type = args[1]
            args = args[2:]
        elif option == '--info':
            if len(args) < 2:
                usage_and_exit()
            parse_file_info(args[1], file_infos)
            args = args[2:]
        elif option == '--quiet':
            quiet = True
            args = args[1:]
        else:
            usage_and_exit()

    if len(args) != 3:
        usage_and_exit()
    bucket_name = args[0]
    local_file = args[1]
    b2_file = args[2]

    console_tool = ConsoleTool()
    info = console_tool.info
    api = console_tool.api
    bucket = api.get_bucket_by_name(bucket_name)

    # Double check that the file is not too big.
    if 5 * 1000 * 1000 * 1000 < os.path.getsize(local_file):
        print 'ERROR: File is bigger that 5GB:', local_file
        sys.exit(1)

    # Compute the SHA1 of the file being uploaded, if it wasn't provided on the command line.
    if sha1_sum is None:
        sha1_sum = hex_sha1_of_file(local_file)

    # Try 5 times to upload the file.  If one fails, get a different
    # upload URL for the next try.
    for i in xrange(5):
        bucket_upload_data = ensure_upload_data(bucket.id_, info, quiet)
        url = bucket_upload_data[StoredAccountInfo.BUCKET_UPLOAD_URL]

        headers = {
            'Authorization': bucket_upload_data[StoredAccountInfo.BUCKET_UPLOAD_AUTH_TOKEN],
            'X-Bz-File-Name': b2_url_encode(b2_file),
            'Content-Type': content_type,
            'X-Bz-Content-Sha1': sha1_sum
            }
        for (k, v) in file_infos.iteritems():
            headers['X-Bz-Info-' + k] = b2_url_encode(v)

        try:
            response = post_file(url, headers, local_file, exit_on_error=False, progress_bar=(not quiet))
            print json.dumps(response, indent=4, sort_keys=True)
            if (not quiet) and ('fileId' in response):
                print "URL by file name: " + info.get_download_url() + "/file/" + bucket_name + "/" + b2_file
                print "URL by fileId: " + info.get_download_url() + "/b2api/v1/b2_download_file_by_id?fileId=" + response['fileId']
            return
        except urllib2.HTTPError as e:
            if 500 <= e.code and e.code < 600:
                info.clear_bucket_upload_data(bucket.id_)
            else:
                report_http_error_and_exit(e, url, None, headers)

    print 'FAILED to upload after 5 tries'
    sys.exit(1)


def download_file_from_url(url, request_body, encoded_headers, local_file_name, progress_bar=False):
    with OpenUrl(url, request_body, encoded_headers) as response:
        info = response.info()
        file_size = int(info['content-length'])
        file_sha1 = info['x-bz-content-sha1']
        print 'File name:   ', info['x-bz-file-name']
        print 'File size:   ', file_size
        print 'Content type:', info['content-type']
        print 'Content sha1:', file_sha1
        for name in info:
            if name.startswith('x-bz-info-'):
                print 'INFO', name[10:] + ':', info[name]
        block_size = 4096
        digest = hashlib.sha1()
        bytes_read = 0

        stream = open(local_file_name, 'wb')
        if PROGRESS_BAR_SUPPORT and progress_bar:
            stream = StreamWithProgress(stream, desc=local_file_name, total=file_size)
        with stream as f:
            while True:
                data = response.read(block_size)
                if len(data) == 0:
                    break
                f.write(data)
                digest.update(data)
                bytes_read += len(data)
        if bytes_read != int(info['content-length']):
            print 'ERROR: only %d of %d bytes read' % (bytes_read, file_size)
        if digest.hexdigest() != file_sha1:
            print 'ERROR: sha1 checksum mismatch -- bad data'
        print 'checksum matches'


def download_file_by_id(args):
    if len(args) != 2:
        usage_and_exit()

    file_id = args[0]
    local_file_name = args[1]

    info = StoredAccountInfo()
    auth_token = info.get_account_auth_token()

    url = url_for_api(info, 'b2_download_file_by_id')
    headers = { 'Authorization' : auth_token }
    params = { 'fileId' : file_id }

    request_body = json.dumps(params)

    download_file_from_url(url, request_body, headers, local_file_name, progress_bar=True)


def download_file_by_name(args):
    if len(args) != 3:
        usage_and_exit()

    bucket_name = args[0]
    file_name = args[1]
    local_file_name = args[2]

    info = StoredAccountInfo()
    auth_token = info.get_account_auth_token()

    url = info.get_download_url() + '/file/' + b2_url_encode(bucket_name) + '/' + b2_url_encode(file_name)
    headers = { 'Authorization' : auth_token }

    download_file_from_url(url, None, headers, local_file_name, progress_bar=True)


def print_ls_entry(is_long, is_folder, name, file):
    # if not long, it's easy
    if not is_long:
        print name
    else:
        # order is file_id, action, date, time, size, name
        format = '%83s  %6s  %10s  %8s  %9d  %s'
        if is_folder:
            print format % ('-', '-', '-', '-', 0, name)
        else:
            file_id = file['fileId']
            action = file['action']
            dt = datetime.datetime.utcfromtimestamp(file['uploadTimestamp'] / 1000)
            date_str = dt.strftime('%Y-%m-%d')
            time_str = dt.strftime('%H:%M:%S')
            size = file['size']
            print format % (file_id, action, date_str, time_str, size, name)

def ls(args):
    # Parse arguments
    long_format = False
    show_versions = False
    while len(args) != 0 and args[0][0] == '-':
        option = args[0]
        args = args[1:]
        if option == '--long':
            long_format = True
        elif option == '--versions':
            show_versions = True
        else:
            print 'Unknown option:', option
            usage_and_exit()
    if len(args) < 1 or 2 < len(args):
        usage_and_exit()
    bucket_name = args[0]
    if len(args) == 1:
        prefix = ""
    else:
        prefix = args[1]
        if not prefix.endswith('/'):
            prefix += '/'

    console_tool = ConsoleTool()
    info = console_tool.info
    api = console_tool.api
    bucket = api.get_bucket_by_name(bucket_name)
    auth_token = info.get_account_auth_token()

    # Loop until all files in the named directory have been listed.
    # The starting point of the first list_file_names request is the
    # prefix we're looking for.  The prefix ends with '/', which is
    # now allowed for file names, so no file name will match exactly,
    # but the first one after that point is the first file in that
    # "folder".   If the first search doesn't produce enough results,
    # then we keep callig list_file_names until we get all of the
    # names in this "folder".
    current_dir = None
    if show_versions:
        api_name = 'b2_list_file_versions'
    else:
        api_name = 'b2_list_file_names'
    start_file_name = prefix
    start_file_id = None
    while True:
        url = url_for_api(info, api_name)
        params = {
            'bucketId' : bucket.id_,
            'startFileName' : start_file_name
            }
        if start_file_id is not None:
            params['startFileId'] = start_file_id
        response = post_json(url, params, auth_token)
        for file in response['files']:
            name = file['fileName']
            if not name.startswith(prefix):
                # We're past the files we care about
                return
            after_prefix = name[len(prefix):]
            if '/' not in after_prefix:
                # This is not a folder, so we'll print it out and
                # continue on.
                file_id = file['fileId']
                size = file['size']
                print_ls_entry(long_format, False, name, file)
                current_dir = None
            else:
                # This is a folder.  If it's different than the folder
                # we're already in, then we can print it.  This check
                # is needed, because all of the files in the folder
                # will be in the list.
                folder_with_slash = after_prefix.split('/')[0] + '/'
                if folder_with_slash != current_dir:
                    folder_name = prefix + folder_with_slash
                    print_ls_entry(long_format, True, folder_name, file)
                    current_dir = folder_with_slash
        if response['nextFileName'] is None:
            # The response says there are no more files in the bucket,
            # so we can stop.
            return

        # Now we need to set up the next search.  The response from
        # B2 has the starting point to continue with the next file,
        # but if we're in the middle of a "folder", we can skip ahead
        # to the end of the folder.  The character after '/' is '0',
        # so we'll replace the '/' with a '0' and start there.
        if current_dir is None:
            start_file_name = response.get('nextFileName')
            start_file_id = response.get('nextFileId')
        else:
            start_file_name = max(
                response['nextFileName'],
                prefix + current_dir[:-1] + '0'
                )

class ConsoleTool(object):
    def __init__(self):
        info = StoredAccountInfo()
        self.api = B2Api(info, AuthInfoCache(info))

    def create_bucket(self, args):
        if len(args) != 2:
            usage_and_exit()
        bucket_name = args[0]
        bucket_type = args[1]

        print self.api.create_bucket(bucket_name, bucket_type).id_

    def delete_bucket(self, args):
        if len(args) != 1:
            usage_and_exit()
        bucket_name = args[0]

        bucket = self.api.get_bucket_by_name(bucket_name)
        response = self.api.delete_bucket(bucket)

        print json.dumps(response, indent=4, sort_keys=True)

    def update_bucket(self, args):
        if len(args) != 2:
            usage_and_exit()
        bucket_name = args[0]
        bucket_type = args[1]

        bucket = self.api.get_bucket_by_name(bucket_name)
        response = bucket.set_type(bucket_type)

        print json.dumps(response, indent=4, sort_keys=True)

    def list_buckets(self, args):
        if len(args) != 0:
            usage_and_exit()

        for b in self.api.list_buckets():
            print '%s  %-10s  %s' % (b.id_, b.type_, b.name)

    def delete_file_version(self, args):
        if len(args) != 2:
            usage_and_exit()
        file_name = args[0]
        file_id = args[1]

        file_info = self.api.delete_file_version(file_id, file_name)

        response = file_info.as_dict()

        print json.dumps(response, indent=2, sort_keys=True)

    def hide_file(self, args):
        if len(args) != 2:
            usage_and_exit()
        bucket_name = args[0]
        file_name = args[1]

        bucket = self.api.get_bucket_by_name(bucket_name)
        file_info = bucket.hide_file(file_name)

        response = file_info.as_dict()

        print json.dumps(response, indent=2, sort_keys=True)

    def make_url(self, args):
        if len(args) != 1:
            usage_and_exit()

        file_id = args[0]

        print self.api.make_url(file_id)

    @property
    def info(self):  # TODO: this is only temporary, remove it
        return self.api.account_info


def main():
    if len(sys.argv) < 2:
        usage_and_exit()

    decoded_argv = decode_sys_argv()

    action = decoded_argv[1]
    args = decoded_argv[2:]

    ct = ConsoleTool()
    try:
        if action == 'authorize_account':
            authorize_account(args)
        elif action == 'clear_account':
            clear_account(args)
        elif action == 'create_bucket':
            ct.create_bucket(args)
        elif action == 'delete_bucket':
            ct.delete_bucket(args)
        elif action == 'delete_file_version':
            ct.delete_file_version(args)
        elif action == 'download_file_by_id':
            download_file_by_id(args)
        elif action == 'download_file_by_name':
            download_file_by_name(args)
        elif action == 'get_file_info':
            get_file_info(args)
        elif action == 'hide_file':
            ct.hide_file(args)
        elif action == 'list_buckets':
            ct.list_buckets(args)
        elif action == 'list_file_names':
            list_file_names(args)
        elif action == 'list_file_versions':
            list_file_versions(args)
        elif action == 'ls':
            ls(args)
        elif action == 'make_url':
            ct.make_url(args)
        elif action == 'update_bucket':
            ct.update_bucket(args)
        elif action == 'upload_file':
            upload_file(args)
        elif action == 'version':
            print 'b2 command line tool, version', VERSION
        else:
            usage_and_exit()
    except B2Error as e:  # TODO: put it somewhere else (decorator of AbstractConsoleCommand.handle()?)
        print e
        sys.exit(1)

if __name__ == '__main__':
    main()
